{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a127429-4232-4326-bf13-f70f3ecd7b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    " from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ed104be-64f1-4c11-9bf4-f8de632ede01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to extract Brand name\n",
    "def get_brand(soup):\n",
    "\n",
    "    try:\n",
    "        brand = soup.find(\"tr\", attrs={'class':'a-spacing-small po-brand'}).text.replace('Brand','').strip()\n",
    "\n",
    "    except AttributeError:\n",
    "        brand = \"\"\n",
    "\n",
    "    return brand\n",
    "    \n",
    "# Function to extract Product Title\n",
    "def get_title(soup):\n",
    "\n",
    "    try:\n",
    "        # Outer Tag Object\n",
    "        title = soup.find(\"span\", attrs={\"id\":'productTitle'})\n",
    "        \n",
    "        # Inner NavigatableString Object\n",
    "        title_value = title.text\n",
    "\n",
    "        # Title as a string value\n",
    "        title_string = title_value.strip()\n",
    "\n",
    "    except AttributeError:\n",
    "        title_string = \"\"\n",
    "\n",
    "    return title_string\n",
    "\n",
    "# Function to extract Product Price\n",
    "def get_price(soup):\n",
    "\n",
    "    try:\n",
    "        price = soup.find(\"span\", attrs={'class':'a-offscreen'}).text.strip()\n",
    "\n",
    "    except AttributeError:\n",
    "        price = \"\"\n",
    "\n",
    "    return price\n",
    "\n",
    "#Function to extract feature \n",
    "def get_feature(soup):\n",
    "\n",
    "    try:\n",
    "        label = soup.find('span', string='Filter Type')\n",
    "        feature = label.find_next('span', class_='a-size-base handle-overflow').text.strip()\n",
    "        \n",
    "    except AttributeError:\n",
    "        feature = \"\"\n",
    "\n",
    "    return feature\n",
    "\n",
    "# Function to extract Product Rating\n",
    "def get_rating(soup):\n",
    "\n",
    "    try:\n",
    "        rating = soup.find(\"i\", attrs={'class':'a-icon a-icon-star a-star-4-5'}).string.strip()\n",
    "    \n",
    "    except AttributeError:\n",
    "        try:\n",
    "            rating = soup.find(\"span\", attrs={'class':'a-icon-alt'}).string.strip()\n",
    "        except:\n",
    "            rating = \"\"\t\n",
    "\n",
    "    return rating\n",
    "\n",
    "# Function to extract Number of User Reviews\n",
    "def get_review_count(soup):\n",
    "    try:\n",
    "        review_count = soup.find(\"span\", attrs={'id':'acrCustomerReviewText'}).string.strip()\n",
    "\n",
    "    except AttributeError:\n",
    "        review_count = \"\"\t\n",
    "\n",
    "    return review_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbeb3d5-b4a1-44a4-af7f-3cca0bb3b66a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1...\n",
      "Scraping page 2...\n",
      "Failed to scrape product page: https://www.amazon.in/sspa/click?ie=UTF8&spc=MToxMjU5MzkzMjQxNjM3NDQ0OjE3NTQ0NjEwODU6c3BfYnRmOjMwMDY0Mjg0MDk0MDEzMjo6MDo6&url=%2FPhilips-AC0920-Purifier-Allergens-Bedrooms%2Fdp%2FB0DB83LFM3%2Fref%3Dsr_1_38_sspa%3Fcrid%3D1PLUGLCS6W9O2%26dib%3DeyJ2IjoiMSJ9.wNu8nDfZ78iRnPGmlS4b4Zvg0R9f26u0xvNb9xSwQ0qyzMs0vzgNj8tsQHcITlG9z9FvFdg93EiLBi0alGMJwFkSQDYtxS2bn0rw9zcTtIZdzqJ8max6HxykVbcCg0OkXZa29juSVVqLoyVEyp6veyLPH8eqZVuxWNnzDlnJlpmBWBh3ixMjAcqaZ95yjepwei_9P7KfarC6X7ytRo307J1fM2anxzB_YhohDNUC11A.KMd8T7BEZTcqy3jvJa1YORiuQD9RnEH4i-_ZxCa0hNY%26dib_tag%3Dse%26keywords%3Dair%2Bpurifier%2Bfor%2Bbedroom%26qid%3D1754461085%26refinements%3Dp_72%253A1318476031%26rnid%3D1318475031%26sprefix%3Dair%2Bpurifier%2Bfor%2Bbedroom%252Caps%252C290%26sr%3D8-38-spons%26sp_csd%3Dd2lkZ2V0TmFtZT1zcF9idGY%26psc%3D1&sp_cr=ZAZ - (\"Connection broken: ConnectionAbortedError(10053, 'An established connection was aborted by the software in your host machine', None, 10053, None)\", ConnectionAbortedError(10053, 'An established connection was aborted by the software in your host machine', None, 10053, None))\n",
      "Scraping page 3...\n",
      "Failed to fetch search results page 3: HTTPSConnectionPool(host='www.amazon.in', port=443): Max retries exceeded with url: /s?k=air+purifier+for+bedroom&rh=p_72%3A1318476031&dc&crid=1PLUGLCS6W9O2&qid=1754391955&rnid=1318475031&sprefix=air+purifier+for+bedroom%2Caps%2C290&ref=sr_nr_p_72_1&ds=v1%3AUZEN1LydFH6Jj%2B7ks35touXvZqcwwTh00ZIN%2Bhrisd8&page=3 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001F137898A50>: Failed to resolve 'www.amazon.in' ([Errno 11001] getaddrinfo failed)\"))\n",
      "Scraping page 4...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[3]\u001B[39m\u001B[32m, line 29\u001B[39m\n\u001B[32m     27\u001B[39m product_url = \u001B[33m\"\u001B[39m\u001B[33mhttps://www.amazon.in\u001B[39m\u001B[33m\"\u001B[39m + link\n\u001B[32m     28\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m29\u001B[39m     new_webpage = \u001B[43mrequests\u001B[49m\u001B[43m.\u001B[49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43mproduct_url\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mheaders\u001B[49m\u001B[43m=\u001B[49m\u001B[43mHEADERS\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     30\u001B[39m     new_soup = BeautifulSoup(new_webpage.content, \u001B[33m\"\u001B[39m\u001B[33mhtml.parser\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     32\u001B[39m     d[\u001B[33m'\u001B[39m\u001B[33mBrand\u001B[39m\u001B[33m'\u001B[39m].append(get_brand(new_soup))\n",
      "\u001B[36mFile \u001B[39m\u001B[32mk:\\Anaconda\\Lib\\site-packages\\requests\\api.py:73\u001B[39m, in \u001B[36mget\u001B[39m\u001B[34m(url, params, **kwargs)\u001B[39m\n\u001B[32m     62\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mget\u001B[39m(url, params=\u001B[38;5;28;01mNone\u001B[39;00m, **kwargs):\n\u001B[32m     63\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33mr\u001B[39m\u001B[33;03m\"\"\"Sends a GET request.\u001B[39;00m\n\u001B[32m     64\u001B[39m \n\u001B[32m     65\u001B[39m \u001B[33;03m    :param url: URL for the new :class:`Request` object.\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m     70\u001B[39m \u001B[33;03m    :rtype: requests.Response\u001B[39;00m\n\u001B[32m     71\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m73\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mget\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m=\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mk:\\Anaconda\\Lib\\site-packages\\requests\\api.py:59\u001B[39m, in \u001B[36mrequest\u001B[39m\u001B[34m(method, url, **kwargs)\u001B[39m\n\u001B[32m     55\u001B[39m \u001B[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001B[39;00m\n\u001B[32m     56\u001B[39m \u001B[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001B[39;00m\n\u001B[32m     57\u001B[39m \u001B[38;5;66;03m# cases, and look like a memory leak in others.\u001B[39;00m\n\u001B[32m     58\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m sessions.Session() \u001B[38;5;28;01mas\u001B[39;00m session:\n\u001B[32m---> \u001B[39m\u001B[32m59\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43msession\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl\u001B[49m\u001B[43m=\u001B[49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mk:\\Anaconda\\Lib\\site-packages\\requests\\sessions.py:589\u001B[39m, in \u001B[36mSession.request\u001B[39m\u001B[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001B[39m\n\u001B[32m    584\u001B[39m send_kwargs = {\n\u001B[32m    585\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mtimeout\u001B[39m\u001B[33m\"\u001B[39m: timeout,\n\u001B[32m    586\u001B[39m     \u001B[33m\"\u001B[39m\u001B[33mallow_redirects\u001B[39m\u001B[33m\"\u001B[39m: allow_redirects,\n\u001B[32m    587\u001B[39m }\n\u001B[32m    588\u001B[39m send_kwargs.update(settings)\n\u001B[32m--> \u001B[39m\u001B[32m589\u001B[39m resp = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43msend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprep\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43msend_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    591\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m resp\n",
      "\u001B[36mFile \u001B[39m\u001B[32mk:\\Anaconda\\Lib\\site-packages\\requests\\sessions.py:746\u001B[39m, in \u001B[36mSession.send\u001B[39m\u001B[34m(self, request, **kwargs)\u001B[39m\n\u001B[32m    743\u001B[39m         \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[32m    745\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m stream:\n\u001B[32m--> \u001B[39m\u001B[32m746\u001B[39m     \u001B[43mr\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcontent\u001B[49m\n\u001B[32m    748\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m r\n",
      "\u001B[36mFile \u001B[39m\u001B[32mk:\\Anaconda\\Lib\\site-packages\\requests\\models.py:902\u001B[39m, in \u001B[36mResponse.content\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    900\u001B[39m         \u001B[38;5;28mself\u001B[39m._content = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    901\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m902\u001B[39m         \u001B[38;5;28mself\u001B[39m._content = \u001B[33;43mb\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43miter_content\u001B[49m\u001B[43m(\u001B[49m\u001B[43mCONTENT_CHUNK_SIZE\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;129;01mor\u001B[39;00m \u001B[33mb\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    904\u001B[39m \u001B[38;5;28mself\u001B[39m._content_consumed = \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m    905\u001B[39m \u001B[38;5;66;03m# don't need to release the connection; that's been handled by urllib3\u001B[39;00m\n\u001B[32m    906\u001B[39m \u001B[38;5;66;03m# since we exhausted the data.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mk:\\Anaconda\\Lib\\site-packages\\requests\\models.py:820\u001B[39m, in \u001B[36mResponse.iter_content.<locals>.generate\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m    818\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m.raw, \u001B[33m\"\u001B[39m\u001B[33mstream\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m    819\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m820\u001B[39m         \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m.raw.stream(chunk_size, decode_content=\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[32m    821\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m ProtocolError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    822\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m ChunkedEncodingError(e)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mk:\\Anaconda\\Lib\\site-packages\\urllib3\\response.py:1063\u001B[39m, in \u001B[36mHTTPResponse.stream\u001B[39m\u001B[34m(self, amt, decode_content)\u001B[39m\n\u001B[32m   1047\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   1048\u001B[39m \u001B[33;03mA generator wrapper for the read() method. A call will block until\u001B[39;00m\n\u001B[32m   1049\u001B[39m \u001B[33;03m``amt`` bytes have been read from the connection or until the\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m   1060\u001B[39m \u001B[33;03m    'content-encoding' header.\u001B[39;00m\n\u001B[32m   1061\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   1062\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.chunked \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m.supports_chunked_reads():\n\u001B[32m-> \u001B[39m\u001B[32m1063\u001B[39m     \u001B[38;5;28;01myield from\u001B[39;00m \u001B[38;5;28mself\u001B[39m.read_chunked(amt, decode_content=decode_content)\n\u001B[32m   1064\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1065\u001B[39m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_fp_closed(\u001B[38;5;28mself\u001B[39m._fp) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m._decoded_buffer) > \u001B[32m0\u001B[39m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mk:\\Anaconda\\Lib\\site-packages\\urllib3\\response.py:1219\u001B[39m, in \u001B[36mHTTPResponse.read_chunked\u001B[39m\u001B[34m(self, amt, decode_content)\u001B[39m\n\u001B[32m   1216\u001B[39m     amt = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1218\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1219\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_update_chunk_length\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1220\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.chunk_left == \u001B[32m0\u001B[39m:\n\u001B[32m   1221\u001B[39m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mk:\\Anaconda\\Lib\\site-packages\\urllib3\\response.py:1138\u001B[39m, in \u001B[36mHTTPResponse._update_chunk_length\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   1136\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.chunk_left \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   1137\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1138\u001B[39m line = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_fp\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfp\u001B[49m\u001B[43m.\u001B[49m\u001B[43mreadline\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# type: ignore[union-attr]\u001B[39;00m\n\u001B[32m   1139\u001B[39m line = line.split(\u001B[33mb\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m;\u001B[39m\u001B[33m\"\u001B[39m, \u001B[32m1\u001B[39m)[\u001B[32m0\u001B[39m]\n\u001B[32m   1140\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32mk:\\Anaconda\\Lib\\socket.py:719\u001B[39m, in \u001B[36mSocketIO.readinto\u001B[39m\u001B[34m(self, b)\u001B[39m\n\u001B[32m    717\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m(\u001B[33m\"\u001B[39m\u001B[33mcannot read from timed out object\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    718\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m719\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_sock\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrecv_into\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    720\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m timeout:\n\u001B[32m    721\u001B[39m     \u001B[38;5;28mself\u001B[39m._timeout_occurred = \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mk:\\Anaconda\\Lib\\ssl.py:1304\u001B[39m, in \u001B[36mSSLSocket.recv_into\u001B[39m\u001B[34m(self, buffer, nbytes, flags)\u001B[39m\n\u001B[32m   1300\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m flags != \u001B[32m0\u001B[39m:\n\u001B[32m   1301\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m   1302\u001B[39m           \u001B[33m\"\u001B[39m\u001B[33mnon-zero flags not allowed in calls to recv_into() on \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[33m\"\u001B[39m %\n\u001B[32m   1303\u001B[39m           \u001B[38;5;28mself\u001B[39m.\u001B[34m__class__\u001B[39m)\n\u001B[32m-> \u001B[39m\u001B[32m1304\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnbytes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuffer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1305\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1306\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mk:\\Anaconda\\Lib\\ssl.py:1138\u001B[39m, in \u001B[36mSSLSocket.read\u001B[39m\u001B[34m(self, len, buffer)\u001B[39m\n\u001B[32m   1136\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m   1137\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m buffer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1138\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_sslobj\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbuffer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1139\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   1140\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._sslobj.read(\u001B[38;5;28mlen\u001B[39m)\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # add your user agent \n",
    "        HEADERS = ({\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36\",'Accept-Language':'en-US, en;q=0.5'})\n",
    "\n",
    "    # The webpage URL\n",
    "    URL = 'https://www.amazon.in/s?k=air+purifier+for+bedroom&rh=p_72%3A1318476031&dc&crid=1PLUGLCS6W9O2&qid=1754391955&rnid=1318475031&sprefix=air+purifier+for+bedroom%2Caps%2C290&ref=sr_nr_p_72_1&ds=v1%3AUZEN1LydFH6Jj%2B7ks35touXvZqcwwTh00ZIN%2Bhrisd8'\n",
    "\n",
    "    d = {\"Brand\":[],\"Title\":[], \"Price\":[],\"Feature\":[], \"Rating\":[], \"Reviews\":[]}\n",
    "\n",
    "    \n",
    "    for page in range(1, 20):  # Scrape first 5 pages; increase range as needed\n",
    "        print(f\"Scraping page {page}...\")\n",
    "        url = f\"https://www.amazon.in/s?k=air+purifier+for+bedroom&rh=p_72%3A1318476031&dc&crid=1PLUGLCS6W9O2&qid=1754391955&rnid=1318475031&sprefix=air+purifier+for+bedroom%2Caps%2C290&ref=sr_nr_p_72_1&ds=v1%3AUZEN1LydFH6Jj%2B7ks35touXvZqcwwTh00ZIN%2Bhrisd8&page={page}\"\n",
    "        \n",
    "        try:\n",
    "            webpage = requests.get(url, headers=HEADERS)\n",
    "            soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "\n",
    "            links = soup.find_all('a', attrs={\n",
    "                'class': \"a-link-normal s-line-clamp-2 s-line-clamp-3-for-col-12 s-link-style a-text-normal\"\n",
    "            })\n",
    "\n",
    "            links_list = [link.get('href') for link in links]\n",
    "\n",
    "            for link in links_list:\n",
    "                product_url = \"https://www.amazon.in\" + link\n",
    "                try:\n",
    "                    new_webpage = requests.get(product_url, headers=HEADERS)\n",
    "                    new_soup = BeautifulSoup(new_webpage.content, \"html.parser\")\n",
    "\n",
    "                    d['Brand'].append(get_brand(new_soup))\n",
    "                    d['Title'].append(get_title(new_soup))\n",
    "                    d['Price'].append(get_price(new_soup))\n",
    "                    d['Feature'].append(get_feature(new_soup))\n",
    "                    d['Rating'].append(get_rating(new_soup))\n",
    "                    d['Reviews'].append(get_review_count(new_soup))\n",
    "\n",
    "                    time.sleep(1)  # Delay to reduce risk of blocking\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to scrape product page: {product_url} - {e}\")\n",
    "                    continue\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to fetch search results page {page}: {e}\")\n",
    "            continue\n",
    "\n",
    "    Amazon_df = pd.DataFrame.from_dict(d)\n",
    "    Amazon_df['Title'].replace('', np.nan, inplace=True)\n",
    "    Amazon_df = Amazon_df.dropna(subset=['Title'])\n",
    "    Amazon_df.to_csv(\"Amazon_data_v2.csv\", header=True, index=False)\n",
    "    print(\"Scraping complete. Data saved to Amazon_data.csv.\")"
   ]
  },
  
